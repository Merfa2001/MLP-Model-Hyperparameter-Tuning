
# MLP Model Hyperparameter Tuning

This repository contains code for training and evaluating Multi-Layer Perceptron (MLP) models on a given dataset. It focuses on experimenting with different hyperparameters, including the number of neurons in hidden layers, optimizer selection, batch normalization, dropout rate, and L2 regularization. The main goal is to assess how these hyperparameters affect the performance of the MLP on a validation set.

## Repository Name: `mlp-hyperparameter-tuning`

## Code Overview

This project involves several key functions:

1. **build_and_train_mlp**: 
   This function builds and trains an MLP model based on the provided hyperparameters. It allows the user to specify the number of neurons, optimizer, regularization methods, and whether to use batch normalization or dropout.

2. **experiment_neurons**: 
   This function runs experiments on different neuron configurations (number of neurons in the hidden layers) to determine how the number of neurons influences model performance. It evaluates models on validation data and stores results for further analysis.

3. **plot_results**:
   This function visualizes the validation accuracy and loss for different experiments. It generates plots comparing different hyperparameter settings to help identify the best configuration.

## Requirements

To run the code, you'll need the following Python libraries:

- TensorFlow
- Matplotlib
- NumPy
- Pandas (if you are working with datasets)

You can install the required dependencies using the following command:

```
pip install tensorflow matplotlib numpy pandas
```

## Experiment Setup

The experiments conducted in this repository are based on varying the following hyperparameters:

- **Number of Neurons**: The number of neurons in the hidden layers of the MLP.
- **Hidden Layers**: The number of hidden layers used in the MLP.
- **Optimizer**: The optimizer used for training the model (SGD, Adam, or RMSprop).
- **Batch Normalization**: Whether or not to include batch normalization layers.
- **Dropout Rate**: The dropout rate applied to the hidden layers to prevent overfitting.
- **L2 Regularization**: The L2 regularization applied to the weights of the model.

## Functions

- `build_and_train_mlp`: This function is responsible for constructing and training the MLP model.
- `experiment_neurons`: This function performs experiments with different neuron configurations and records the results.
- `plot_results`: This function generates plots for the validation accuracy and loss for various hyperparameter settings.

## Usage

### Step 1: Prepare your data
Ensure that you have prepared your data for training and validation (`X_train`, `y_train`, `X_val`, `y_val`).

### Step 2: Run Experiments

```python
# Example: Evaluate the effect of the number of neurons on a model with one hidden layer using SGD optimizer
results_1_layer = experiment_neurons(neuron_configs, X_train_scaled, y_train, X_val_scaled, y_val, hidden_layers=1, optimizer='SGD')
```

### Step 3: Plot the Results

```python
# Example: Plot the results for experiments with one hidden layer and SGD optimizer
plot_results(results_1_layer, "Single Hidden Layer - SGD")
```

### Step 4: Analyze the Results

You can analyze the plots generated by `plot_results` to understand how different hyperparameters affect your model's performance.

## License

This project is licensed under the MIT License.

